{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Problem 4b: Linear Regression with multiple variables\n",
    "## Experiments with the Boston housing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions \n",
    "This file contains code that helps you get started on linear regression with many variables. You will need to complete functions in **linear_regressor_multi.py** and **utils.py**. The only changes to make in this notebook are marked with **TODO**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "will start by loading and displaying some values from the full Boston housing dataset with thirteen features of census tracts that are believed to be predictive of the median home price in the tract (see **housing.names.txt** for a full description of these features). By looking at the values, you will note that the values of some of the features are  about 1000 times the values of others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikey/miniforge3/envs/enc_tensorflow/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "0       15.3  396.90   4.98  \n",
       "1       17.8  396.90   9.14  \n",
       "2       17.8  392.83   4.03  \n",
       "3       18.7  394.63   2.94  \n",
       "4       18.7  396.90   5.33  \n",
       "..       ...     ...    ...  \n",
       "501     21.0  391.99   9.67  \n",
       "502     21.0  396.90   9.08  \n",
       "503     21.0  396.90   5.64  \n",
       "504     21.0  393.45   6.48  \n",
       "505     21.0  396.90   7.88  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plot_utils, utils\n",
    "from linear_regressor_multi import LinearRegressor_Multi, LinearReg_SquaredLoss\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print('Reading data ...')\n",
    "bdata = load_boston()\n",
    "df = pd.DataFrame(data = bdata.data, columns = bdata.feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with multiple variables\n",
    "When features differ by orders of magnitude, feature scaling becomes important\n",
    "to make  gradient descent converge  quickly.\n",
    "**Your task here is to complete the code in feature_normalize.py in utils.py**. \n",
    "- First, subtract the mean value of each feature from the dataset. \n",
    "- Second, divide the feature values by their respective standard deviations. The standard deviation is a way of measuring how much variation there is in the range of values of a particular feature (most data points will lie within two standard deviations of the mean).  \n",
    "\n",
    "You will do this for all the features and your code should work with\n",
    "datasets of all sizes (any number of features/examples). Note that each\n",
    "column of the matrix X corresponds to one feature.\n",
    "When normalizing the features, it is important\n",
    "to store the values used for normalization - the mean value and the standard deviation used for the computations.\n",
    "\n",
    "Then, run the computation in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.41978194  0.28482986 -1.2879095  ... -1.45900038  0.44105193\n",
      "  -1.0755623 ]\n",
      " [-0.41733926 -0.48772236 -0.59338101 ... -0.30309415  0.44105193\n",
      "  -0.49243937]\n",
      " [-0.41734159 -0.48772236 -0.59338101 ... -0.30309415  0.39642699\n",
      "  -1.2087274 ]\n",
      " ...\n",
      " [-0.41344658 -0.48772236  0.11573841 ...  1.17646583  0.44105193\n",
      "  -0.98304761]\n",
      " [-0.40776407 -0.48772236  0.11573841 ...  1.17646583  0.4032249\n",
      "  -0.86530163]\n",
      " [-0.41500016 -0.48772236  0.11573841 ...  1.17646583  0.44105193\n",
      "  -0.66905833]]\n"
     ]
    }
   ],
   "source": [
    "X = df.values   # (506, 13) -> 13 features\n",
    "y = bdata.target\n",
    "\n",
    "# need to scale the features (use zero mean scaling)\n",
    "X_norm, mu, sigma = utils.feature_normalize(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function and gradient descent (vectorized)\n",
    "Previously, you implemented gradient descent on a univariate regression problem. The only difference now is that there are more features in the matrix X. The hypothesis function and the batch gradient descent update\n",
    "rule remain unchanged. You should complete the code for the train method and the loss method at the indicated points\n",
    "in **linear_regressor_multi.py** to implement the cost function and gradient descent for linear regression with\n",
    "multiple variables.  Make sure your code supports any number of features and that it is **vectorized**.\n",
    "I recommend the use of  numpy's code vectorization facilities. You should see the cost $J(\\theta)$ converge as shown in Figure 5 of the assignment handout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running gradient descent ..\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta computed by gradient descent: [ 2.25328063e+01 -9.21597716e-01  1.07007562e+00  1.05469840e-01\n",
      "  6.86800441e-01 -2.05006613e+00  2.68075474e+00  1.39444430e-02\n",
      " -3.10628281e+00  2.57282283e+00 -1.97535499e+00 -2.05719700e+00\n",
      "  8.48665373e-01 -3.74016957e+00]\n"
     ]
    }
   ],
   "source": [
    "# add intercept term to X_norm\n",
    "\n",
    "XX = np.vstack([np.ones((X.shape[0],)),X_norm.T]).T\n",
    "\n",
    "print('Running gradient descent ..')\n",
    "\n",
    "# set up model and train \n",
    "\n",
    "linear_reg3 = LinearReg_SquaredLoss()\n",
    "J_history3 = linear_reg3.train(XX,y,learning_rate=0.01,num_iters=5000,verbose=False)\n",
    "\n",
    "# Plot the convergence graph and show it (or save it in fig5.pdf)\n",
    "\n",
    "plot_utils.plot_data(range(len(J_history3)),J_history3,'Number of iterations','Cost J')\n",
    "plt.show()\n",
    "\n",
    "# Display the computed theta\n",
    "\n",
    "print('Theta computed by gradient descent: %s' % (linear_reg3.theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on unseen data points\n",
    "After learning the parameter $\\theta$, we  want to predict the median home prices for new census tracts. \n",
    "Given the thirteen characteristics x of a new census tract, we must first normalize x using the mean and standard deviations that we had previously computed from the training set. Then, we take the dot product of the normalized x (with a 1 prepended (to account for the intercept term) with the parameter vector $\\theta$ to make a prediction.\n",
    "In the cell below, your  final parameter values for $\\theta$ will  be used to make predictions on median home values\n",
    "for an average census tract, characterized by average values for all the thirteen features.  Complete the\n",
    "calculation in  the indicated lines below. Now run this cell to see what the  prediction of median home value for an average tract is. Remember to scale the features correctly for this prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For average home in Boston suburbs, we predict a median home value of 225328.06\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# TODO:                                                                #\n",
    "# Predict values for the average home                                  #\n",
    "# remember to multiply prediction by 10000 using linear_reg3           #\n",
    "#   One line of code expected; replace pred_cost = 0 line              # \n",
    "########################################################################\n",
    "\n",
    "X_test1 = X.mean(axis=0)  # (1,13) \n",
    "X_test1_scaled = (X_test1 - mu)/sigma # Scale the features -> Still need to add 1 padding\n",
    "X_test1_scaled = np.hstack((np.array((1)), X_test1_scaled ))\n",
    "pred_cost = 10000 * linear_reg3.predict(X_test1_scaled)\n",
    "print('For average home in Boston suburbs, we predict a median home value of %.2f' % (pred_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal equations\n",
    "Using the closed form solution for $\\theta$ does not require any feature scaling, and you will get\n",
    "an exact solution in one calculation: there is no loop until convergence\n",
    "as in gradient descent.\n",
    "Complete the code in the method *normal_eqn* in *linear_regressor_multi.py* to  calculate $\\theta$. Now make a prediction for the average census tract (same example as in the previous problem). Do the predictions match up? Remember that while you do not need to scale your features, you still\n",
    "need to add a 1 to the example to have an intercept term ($\\theta_0$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta computed by direct solution is: [ 3.64594884e+01 -1.08011358e-01  4.64204584e-02  2.05586264e-02\n",
      "  2.68673382e+00 -1.77666112e+01  3.80986521e+00  6.92224640e-04\n",
      " -1.47556685e+00  3.06049479e-01 -1.23345939e-02 -9.52747232e-01\n",
      "  9.31168327e-03 -5.24758378e-01]\n",
      "For average home in Boston suburbs, we predict a median home value of 225328.06\n"
     ]
    }
   ],
   "source": [
    "X = df.values\n",
    "y = bdata.target\n",
    "XX1 = np.vstack([np.ones((X.shape[0],)),X.T]).T\n",
    "\n",
    "linear_reg4 = LinearReg_SquaredLoss()\n",
    "\n",
    "theta_n = linear_reg4.normal_equation(XX1,y)\n",
    "\n",
    "print('Theta computed by direct solution is: %s' % (theta_n))\n",
    "\n",
    "########################################################################\n",
    "# TODO:                                                                #\n",
    "# Predict values for the average home using theta_n                    #\n",
    "# remember to multiply prediction by 10000                             #\n",
    "#   One line of code expected; replace pred_cost = 0 line              # \n",
    "########################################################################\n",
    "X_test2 = X.mean(axis=0)  # (14, ) \n",
    "X_test2 = np.hstack(( np.array((1)), X_test2 ))\n",
    "\n",
    "pred_cost = 10000 * np.matmul(theta_n, X_test2)\n",
    "print('For average home in Boston suburbs, we predict a median home value of %.2f' % (pred_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring convergence of gradient descent\n",
    "In this part of the exercise, you will get to try out different learning rates for\n",
    "the dataset and find a learning rate that converges quickly. You can change\n",
    "the learning rate and the number of iterations by modifying the call to the **LinearReg** constructor in the cell below.\n",
    "The next phase will call your train function and run gradient descent  at the chosen learning\n",
    "rate for the chosen number of iterations. The function should also return the history of $J(\\theta)$ values in a vector\n",
    "$J$. After the last iteration, the  script plots the $J$ values against\n",
    "the number of the iterations.\n",
    "If you picked a learning rate within a good range, your plot should look similar\n",
    "to Figure 5. If your graph looks very different, especially if your value of $J(\\theta)$\n",
    "increases or even blows up, adjust your learning rate and try again. We recommend trying values of the learning rate $\\alpha$ on a log-scale, at multiplicative\n",
    "steps of about 3 times the previous value (i.e., 0.3, 0.1, 0.03, 0.01 and so on).\n",
    "You may also want to adjust the number of iterations you are running if that\n",
    "will help you see the overall trend in the curve. Present plots of $J$ as a function of the number of iterations for different learning rates. What are good learning rates and number of iterations for this problem? Include plots and acomments in Gradescope to justify your choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the learning_rate and num_iters in the call below to find the \n",
    "# best learning rate for this data set.\n",
    "\n",
    "learning_rates = [0.01, 0.03, 0.1, 0.3]\n",
    "\n",
    "########################################################################\n",
    "# TODO:                                                                #\n",
    "# Produce convergence plots for gradient descent with the rates above  #\n",
    "# using data (XX,y). Include them in your writeup.                     #\n",
    "#   4-5 lines of code expected                                         #\n",
    "########################################################################\n",
    "model = LinearReg_SquaredLoss()\n",
    "for lr in learning_rates:\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
